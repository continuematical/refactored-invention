## 实现3.5图像分类的实验

实验目的：获取Fashion-Mnist数据集。

Fashion-Mnist数据集包含七万张灰度图像，其中有六万张训练集和一万张测试集。每张图像的大小为28$\times$28。

### 读取数据集

`Pytorch`中的内置框架`torchvision.datasets`中含有服饰数据集，我们通过此函数下载数据集（如下图），并将图像数据转化为浮点数格式。

![](F:\refactored-invention\images\数据类型转换.png)

![数据集读取](F:\refactored-invention\images\数据集读取.png)

### 展示

并展示前十八张图片（如下图）。

![](F:\refactored-invention\images\展示图片.png)

## 实现3.7softmax回归实验

### 算法原理

#### 一般的softmax实现

1. 对于特征集合$X$ ，将其经过线性组合得到预测的模型：
   $$
   f(x;w)=w_1x_1+w_2x_2...w_nx_n+b=w^Tx+b
   $$

2. 为了保证在任何数据上的输出都是非负的，使用softmax激励模型精确地估计概率：
   $$
   \hat y=softmax(o)\;其中\;\hat y_j=\frac {exp(o_j)} {\sum_k exp(o_k)}
   $$

3. 故softmax回归的矢量表达式的计算式为：
   $$
   \begin{cases}
   O=XW+b\\
   \hat Y=softmax(O)
   \end{cases}
   $$
   

4. 定义损失函数：在softmax回归中使用交叉熵损失函数。
   $$
   H[P]=\sum_j-P(j)logP(j)
   $$
   

#### 简洁版的softmax实现

相比于以上的例子，对softmax激励模型进行了修改。

为了避免数值非常大的值影响结果，出现上溢问题。在计算softmax之前，首先减去一个$max(o_k)$。
$$
\hat y_j=\frac {exp(o_j-max(o_k))exp(max(o_k))} {\sum_k exp(o_k-max(o_k))exp(max(o_k))}
$$

### 代码实现与结果

#### 一般的softmax实现

1. 定义softmax操作

![](F:\refactored-invention\images\定义softmax操作.png)

2. 定义模型

![](F:\refactored-invention\images\定义模型.png)

3. 定义损失函数

![](F:\refactored-invention\images\定义损失函数.png)

4. 模型训练

![](F:\refactored-invention\images\一般模型训练.png)

5. 模型预测

![](F:\refactored-invention\images\一般模型预测.png)

#### 简洁版的softmax实现

![](F:\refactored-invention\images\简洁版模型训练.png)