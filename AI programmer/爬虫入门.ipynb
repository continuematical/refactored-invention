{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "261e1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.csdn.net/ChenBinBini/article/details/109739116\n",
    "import urllib.request, urllib.error  # 制定url，获取网页数据\n",
    "import re # 正则表达式\n",
    "from bs4 import BeautifulSoup  # 网页解析，获取数据\n",
    "import xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d4e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "findLink = re.compile(r'<a href=\"(.*?)\">')  # 创建正则表达式对象，标售规则   影片详情链接的规则\n",
    "findImgSrc = re.compile(r'<img.*src=\"(.*?)\"', re.S)\n",
    "findTitle = re.compile(r'<span class=\"title\">(.*)</span>')\n",
    "findRating = re.compile(r'<span class=\"rating_num\" property=\"v:average\">(.*)</span>')\n",
    "findJudge = re.compile(r'<span>(\\d*)人评价</span>')\n",
    "findInq = re.compile(r'<span class=\"inq\">(.*)</span>')\n",
    "findBd = re.compile(r'<p class=\"\">(.*?)</p>', re.S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d64b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到一个指定url的网页内容\n",
    "def askURL(url):\n",
    "    # 请求头\n",
    "    head={\n",
    "        \"User-Agent\": \"Mozilla / 5.0(Windows NT 10.0; Win64; x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 80.0.3987.122  Safari / 537.36\"\n",
    "    }\n",
    "    \n",
    "    request=urllib.request.Request(url, headers=head)\n",
    "    html=\"\"\n",
    "    try:\n",
    "        response=urllib.request.urlopen(request)\n",
    "        html=response.read().decode(\"utf-8\")\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e,\"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e, \"reason\"):\n",
    "            print(e.reason)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60a9fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 爬取网页\n",
    "def getData(baseurl):\n",
    "    datalist = []  #用来存储爬取的网页信息\n",
    "    for i in range(0, 10):  # 调用获取页面信息的函数，10次\n",
    "        url = baseurl + str(i * 25)\n",
    "        html = askURL(url)  # 保存获取到的网页源码\n",
    "        # 2.逐一解析数据\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for item in soup.find_all('div', class_=\"item\"):  # 查找符合要求的字符串\n",
    "            data = []  # 保存一部电影所有信息\n",
    "            item = str(item)\n",
    "            link = re.findall(findLink, item)[0]  # 通过正则表达式查找\n",
    "            data.append(link)\n",
    "            imgSrc = re.findall(findImgSrc, item)[0]\n",
    "            data.append(imgSrc)\n",
    "            titles = re.findall(findTitle, item)\n",
    "            if (len(titles) == 2):\n",
    "                ctitle = titles[0]\n",
    "                data.append(ctitle)\n",
    "                otitle = titles[1].replace(\"/\", \"\")  #消除转义字符\n",
    "                data.append(otitle)\n",
    "            else:\n",
    "                data.append(titles[0])\n",
    "                data.append(' ')\n",
    "            rating = re.findall(findRating, item)[0]\n",
    "            data.append(rating)\n",
    "            judgeNum = re.findall(findJudge, item)[0]\n",
    "            data.append(judgeNum)\n",
    "            inq = re.findall(findInq, item)\n",
    "            if len(inq) != 0:\n",
    "                inq = inq[0].replace(\"。\", \"\")\n",
    "                data.append(inq)\n",
    "            else:\n",
    "                data.append(\" \")\n",
    "            bd = re.findall(findBd, item)[0]\n",
    "            bd = re.sub('<br(\\s+)?/>(\\s+)?', \"\", bd)\n",
    "            bd = re.sub('/', \"\", bd)\n",
    "            data.append(bd.strip())\n",
    "            datalist.append(data)\n",
    "\n",
    "    return datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3589119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据到表格\n",
    "def saveData(datalist,savepath):\n",
    "    print(\"save.......\")\n",
    "    book = xlwt.Workbook(encoding=\"utf-8\",style_compression=0) #创建workbook对象\n",
    "    sheet = book.add_sheet('豆瓣电影Top250', cell_overwrite_ok=True) #创建工作表\n",
    "    col = (\"电影详情链接\",\"图片链接\",\"影片中文名\",\"影片外国名\",\"评分\",\"评价数\",\"概况\",\"相关信息\")\n",
    "    for i in range(0,8):\n",
    "        sheet.write(0,i,col[i])  #列名\n",
    "    for i in range(0,250):\n",
    "        # print(\"第%d条\" %(i+1))       #输出语句，用来测试\n",
    "        data = datalist[i]\n",
    "        for j in range(0,8):\n",
    "            sheet.write(i+1,j,data[j])  #数据\n",
    "    book.save(savepath) #保存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "832fd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    baseurl = \"https://movie.douban.com/top250?start=\"  #要爬取的网页链接\n",
    "    # 1.爬取网页\n",
    "    datalist = getData(baseurl)\n",
    "    savepath = \"豆瓣电影Top250.xls\"    #当前目录新建XLS，存储进去\n",
    "    # 3.保存数据\n",
    "    saveData(datalist,savepath)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e239499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save.......\n",
      "爬取完毕！\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "print(\"爬取完毕！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68ff21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
